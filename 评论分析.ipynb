{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import enchant\n",
    "import nltk\n",
    "import csv\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "##删除文件中的乱码\n",
    "def delete(aaa,number):                       #传入原csv文件每行元素和元素个数\n",
    "    bbb = []\n",
    "    for i in range(0,number):                   #分别对每行中每个元素进行处理\n",
    "        ccc = aaa[i].encode(\"ASCII\",\"ignore\")   #删除乱码，乱码没有对应的ASCII编码\n",
    "        bbb.append(ccc.decode())                #将删除乱码后的字符串重新组成一个列表\n",
    "    return bbb\n",
    "\n",
    "csv_file1=csv.reader(open('complaint1700.csv','r'))    \n",
    "csv_file=csv.writer(open('complaint_new.csv','a',newline=''),dialect='excel')\n",
    "content=[] #用来存储整个文件的数据，存成一个列表，列表的每一个元素又是一个列表，表示的是文件的某一行\n",
    "\n",
    "for line in csv_file1:\n",
    "    number = len(line)\n",
    "    #将整个csv文件处理后组成一个二维列表，该列表每个元素都是一个列表，对应原csv文件的每一行内容（已经删除乱码）\n",
    "    content.append(delete(line,number)) \n",
    "    \n",
    "for i in range(0,len(content)):\n",
    "    csv_file.writerow(content[i])       #写进一个新的csv文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "##删除文件中的乱码\n",
    "def delete(aaa,number):                       #传入原csv文件每行元素和元素个数\n",
    "    bbb = []\n",
    "    for i in range(0,number):                   #分别对每行中每个元素进行处理\n",
    "        ccc = aaa[i].encode(\"ASCII\",\"ignore\")   #删除乱码，乱码没有对应的ASCII编码\n",
    "        bbb.append(ccc.decode())                #将删除乱码后的字符串重新组成一个列表\n",
    "    return bbb\n",
    "\n",
    "csv_file2=csv.reader(open('noncomplaint1700.csv','r'))    \n",
    "csv_file3=csv.writer(open('noncomplaint_new.csv','a',newline=''),dialect='excel')\n",
    "content=[] #用来存储整个文件的数据，存成一个列表，列表的每一个元素又是一个列表，表示的是文件的某一行\n",
    "\n",
    "for line in csv_file2:\n",
    "    number = len(line)\n",
    "    #将整个csv文件处理后组成一个二维列表，该列表每个元素都是一个列表，对应原csv文件的每一行内容（已经删除乱码）\n",
    "    content.append(delete(line,number)) \n",
    "    \n",
    "for i in range(0,len(content)):\n",
    "    csv_file3.writerow(content[i])       #写进一个新的csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "##删除文件中的乱码\n",
    "def delete(aaa,number):                       #传入原csv文件每行元素和元素个数\n",
    "    bbb = []\n",
    "    for i in range(0,number):                   #分别对每行中每个元素进行处理\n",
    "        ccc = aaa[i].encode(\"ASCII\",\"ignore\")   #删除乱码，乱码没有对应的ASCII编码\n",
    "        bbb.append(ccc.decode())                #将删除乱码后的字符串重新组成一个列表\n",
    "    return bbb\n",
    "\n",
    "csv_file4=csv.reader(open('testdata1.csv','r'))    \n",
    "csv_file5=csv.writer(open('testdata_new.csv','a',newline=''),dialect='excel')\n",
    "content=[] #用来存储整个文件的数据，存成一个列表，列表的每一个元素又是一个列表，表示的是文件的某一行\n",
    "\n",
    "for line in csv_file4:\n",
    "    number = len(line)\n",
    "    #将整个csv文件处理后组成一个二维列表，该列表每个元素都是一个列表，对应原csv文件的每一行内容（已经删除乱码）\n",
    "    content.append(delete(line,number)) \n",
    "    \n",
    "for i in range(0,len(content)):\n",
    "    csv_file5.writerow(content[i])       #写进一个新的csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint=pd.read_csv('complaint_new.csv')\n",
    "noncomplaint=pd.read_csv('noncomplaint_new.csv')\n",
    "testdata=pd.read_csv('testdata_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint1=complaint.copy()\n",
    "noncomplaint1=noncomplaint.copy()\n",
    "#将complain和noncomplain的comment垂直组合\n",
    "comment=pd.concat([complaint1,noncomplaint1]).reset_index()\n",
    "#设置label项目，抱怨为0，非抱怨为1\n",
    "target1=np.zeros((complaint1.shape[0],1))\n",
    "target2=np.ones((noncomplaint1.shape[0],1))\n",
    "target=np.vstack((target1,target2))\n",
    "comment['target']=target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment1=comment.copy()\n",
    "testdata1=testdata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将所有的comment合为一个，便于处理\n",
    "def text_join(data):\n",
    "    com=data.tweet.tolist()\n",
    "    a=''\n",
    "    for i in range(len(com)):\n",
    "        a=a+com[i]+' ¥'\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除数据中的@部分、&部分和网址的算法\n",
    "def drop_words(x):\n",
    "    #运用re正则库\n",
    "    a=re.compile(r'@\\w*|\\&\\w*|http:\\/\\/\\w*\\.\\w*\\/\\w*|\\\\')\n",
    "    new_comment=a.sub('',x)\n",
    "    return new_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查更正算法\n",
    "#利用pyenchant文字检查包\n",
    "def spellcheck(x):\n",
    "    from enchant.checker import SpellChecker\n",
    "    #设置为英文\n",
    "    chkr = SpellChecker(\"en_US\")\n",
    "    chkr.set_text(x)\n",
    "    #遍利x中的每一个字符\n",
    "    for j in chkr:\n",
    "        b=j.suggest()\n",
    "        #if没有suggest的单词就不进行替换\n",
    "        if len(b)==0:\n",
    "            j.replace('')\n",
    "        #如果有suggest的单词就以第一个进行替换\n",
    "        else:\n",
    "            j.replace(b[0])\n",
    "    #获取替换后的文本\n",
    "    new_comment1=chkr.get_text()\n",
    "    return new_comment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "#缩略词还原\n",
    "def cont_reback(x):\n",
    "    #编写还原列表\n",
    "    cont_patterns = [\n",
    "        (b\"[^a-zA-Z]u[^a-zA-Z]\", b\"you\"),\n",
    "        (b\"[^a-zA-Z]r[^a-zA-Z]\", b\"are\"),\n",
    "        (b\"[^a-zA-Z]y[^a-zA-Z]\", b\"why\"),\n",
    "        (b\"[^a-zA-Z]b4[^a-zA-Z]\", b\"before\"),\n",
    "        (b'(W|w)on\\'t', b'will not'),\n",
    "        (b'(W|w)ouldn\\'t', b'would not'),\n",
    "        (b'(C|c)an\\'t', b'can not'),\n",
    "        (b'(I|i)\\'m', b'i am'),\n",
    "        (b'(A|a)in\\'t', b'is not'),\n",
    "        (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "        (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "        (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "        (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "        (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "        (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "        (b'(W|w)tf', b'what the fuck')]\n",
    "    #替代函数\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "    #str转化为bytes形式\n",
    "    c=bytes(x,encoding='utf8')\n",
    "    for (pattern, repl) in patterns:\n",
    "        #如果含有这一缩写就进行替换\n",
    "        if pattern.search(c):\n",
    "            c= re.sub(pattern, repl, c)\n",
    "    #bytes转化为str形式\n",
    "    new_comment2=str(c,encoding='utf8')\n",
    "    return new_comment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词形还原\n",
    "def wordlemmatize(x):\n",
    "    #将句子拆分成单个单词\n",
    "    d=str.split(x,' ')\n",
    "    #运用nltk\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    from nltk import pos_tag #词性标签\n",
    "    from nltk import word_tokenize#单词还本\n",
    "    #设置单词和词性返回算法\n",
    "    def lemmatize(token, tag):\n",
    "        if tag[0].lower() in ['n', 'v']:\n",
    "            return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "        return token\n",
    "    #单词还原\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #单词还原的词性列表\n",
    "    tagged_corpus = [pos_tag(word_tokenize(document)) for document in d]\n",
    "    e=[]\n",
    "    #遍历词性列表\n",
    "    for i in tagged_corpus:\n",
    "        for token, tag in i:\n",
    "            f=lemmatize(token, tag)\n",
    "            e.append(f)\n",
    "    #重新转化为句子\n",
    "    new_comment3=' '.join(e)\n",
    "    return new_comment3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除标点符号和数字\n",
    "def exclude_word(x):\n",
    "    #设置要去除的标点符号\n",
    "    exclude=re.compile(r'\\.|\\,|\\;|\\?|\\$|\\d|\\\\|\\/|\\\\\\\\|\\(|\\)|\\:|\\_|\\#')\n",
    "    new_comment4=exclude.sub('',x)\n",
    "    return new_comment4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用TF-IDF进行特征处理\n",
    "def tfidf_demo(x):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    #引入停用词\n",
    "    #from nltk.corpus import stopwords\n",
    "    stopwords=['i','you','we','he','she',\n",
    "               'be','have','they','just','one',\n",
    "               'the','a','aa','aaa','will',\n",
    "               'would','as','to','this','there',\n",
    "               'can','could','of','for']\n",
    "    tf_idf=TfidfVectorizer(stop_words=stopwords)\n",
    "    tfidf_matrix = tf_idf.fit_transform(x)\n",
    "    #返回特征值矩阵，特征值名称，tf_idf生成器\n",
    "    return tfidf_matrix.toarray(),tf_idf.get_feature_names(),tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    comment=text_join(data)\n",
    "    #Step1:删除数据中的@部分、&部分以及网址\n",
    "    new_comment=drop_words(comment)\n",
    "    #Step2:拼写检查更正\n",
    "    new_comment1=spellcheck(new_comment)\n",
    "    #Step3:缩略词还原\n",
    "    new_comment2=cont_reback(new_comment1)\n",
    "    #Step4:词形还原\n",
    "    new_comment3=wordlemmatize(new_comment2)\n",
    "    #Step5:转化为小写\n",
    "    new_comment3=new_comment3.lower()                                                                                       \n",
    "    #Step6:去除标点符号和数字\n",
    "    new_comment4=exclude_word(new_comment3)\n",
    "    comment_new=str.split(new_comment4,'¥')\n",
    "    #Step7:特征处理\n",
    "    feature_matrix,feature_names,tf_idf=tfidf_demo(comment_new)\n",
    "    #返回特征值矩阵，特征值名称，tf_idf生成器\n",
    "    return feature_matrix,feature_names,tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,feature_names,tf_idf=data_preprocessing(comment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelset=np.array(list(comment.target.values)+[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standard_demo(x):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    standard=StandardScaler(with_mean=False)\n",
    "    standard_result=standard.fit_transform(x)\n",
    "    return standard_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Minmax_demo(x):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    minmax=MinMaxScaler()\n",
    "    minmax_result=minmax.fit_transform(x)\n",
    "    return minmax_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_demo(x):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca=PCA(n_components=0.95)\n",
    "    x_pca=pca.fit_transform(x)\n",
    "    return x_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项式贝叶斯\n",
    "1. 最大最小特征化处理\n",
    "2. 划分训练集和测试集，选取75%作为训练集，25%作为测试，附加42个随机值\n",
    "3. 朴素贝叶斯，拉普拉斯平滑系数为20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNB_demo():\n",
    "    ##建立分析模型_朴素贝叶斯\n",
    "    from sklearn.model_selection import  train_test_split\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    x=dataset\n",
    "    y=labelset\n",
    "    x=Minmax_demo(x)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=42)\n",
    "    #使用朴素贝叶斯进行训练\n",
    "    clf = MultinomialNB(alpha=20)   \n",
    "    clf.fit(X_train,y_train)    # 利用训练数据对模型参数进行估计\n",
    "    y_predict = clf.predict(X_test)     # 对参数进行预测\n",
    "    #获取结果报告\n",
    "    print('The Accuracy of Naive Bayes Classifier is:', clf.score(X_test,y_test))\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of Naive Bayes Classifier is: 0.7337278106508875\n"
     ]
    }
   ],
   "source": [
    "mnb=MNB_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林\n",
    "1. 标准分布特征化处理\n",
    "2. 运用主成分分析进行降维\n",
    "3. 划分训练集和测试集，选取75%作为训练集，25%作为测试，附加42个随机值\n",
    "4. 选取100个小决策树进行集成学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_demo():\n",
    "    from sklearn.model_selection import  train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    x=dataset\n",
    "    y=labelset\n",
    "    x=Standard_demo(x)\n",
    "    x=PCA_demo(x)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=42)\n",
    "    #使用朴素贝叶斯进行训练\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train,y_train)    # 利用训练数据对模型参数进行估计\n",
    "    y_predict = clf.predict(X_test)     # 对参数进行预测\n",
    "    #获取结果报告\n",
    "    print('The Accuracy of RandomForest Classifier is:', clf.score(X_test,y_test))\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of RandomForest Classifier is: 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "RandomForest=RandomForest_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "1. 标准分布特征化处理\n",
    "2. 运用主成分分析进行降维\n",
    "3. 划分训练集和测试集，选取75%作为训练集，25%作为测试，附加42个随机值\n",
    "4. 选取k=33（33个排名最靠前的邻居），采用蛮力实现（brute）的方法计算其距离，距离的加权方式为按照距离加权（distance）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_demo():\n",
    "    ##建立分析模型_朴素贝叶斯\n",
    "    from sklearn.model_selection import  train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    x=dataset\n",
    "    y=labelset\n",
    "    x=Standard_demo(x)\n",
    "    x=PCA_demo(x)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(dataset,labelset,test_size=0.25,random_state=42)\n",
    "    clf=KNeighborsClassifier(n_neighbors=33,algorithm='brute',weights='distance')\n",
    "    clf.fit(X_train,y_train)\n",
    "    print('The Accuracy of KNN Classifier is:',clf.score(X_test,y_test))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of KNN Classifier is: 0.7230769230769231\n"
     ]
    }
   ],
   "source": [
    "KNN=KNN_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 投票集成方法\n",
    "1. 标准分布特征化处理\n",
    "2. 运用主成分分析进行降维\n",
    "3. 划分训练集和测试集，选取75%作为训练集，25%作为测试，附加42个随机值\n",
    "4. 选取刚才训练好的三个分类器进行集成\n",
    "5. 投票方式为软投票，即按照比例加权平均，朴素贝叶斯和KNN比重分别为3/7，随机森林比重为1/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_demo():\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    from sklearn.model_selection import  train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "    x=dataset\n",
    "    y=labelset\n",
    "    x=Standard_demo(x)\n",
    "    x=PCA_demo(x)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(dataset,labelset,test_size=0.25,random_state=42)\n",
    "    clf=VotingClassifier(estimators=\n",
    "                         [('mnb',mnb),\n",
    "                          ('RandomForest',RandomForest),\n",
    "                          ('KNN',KNN)],voting='soft',weights=[3,1,3]）\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    print('The Accuracy of voting Classifier is:',clf.score(X_test,y_test))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of voting Classifier is: 0.749112426035503\n"
     ]
    }
   ],
   "source": [
    "vote=vote_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_demo():\n",
    "    comment=text_join(testdata1)\n",
    "    #Step1:删除数据中的@部分、&部分以及网址\n",
    "    new_comment=drop_words(comment)\n",
    "    #Step2:拼写检查更正\n",
    "    new_comment1=spellcheck(new_comment)\n",
    "    #Step3:缩略词还原\n",
    "    new_comment2=cont_reback(new_comment1)\n",
    "    #Step4:词形还原\n",
    "    new_comment3=wordlemmatize(new_comment2)\n",
    "    #Step5:转化为小写\n",
    "    new_comment3=new_comment3.lower()                                                                                       \n",
    "    #Step6:去除标点符号和数字\n",
    "    new_comment4=exclude_word(new_comment3)\n",
    "    comment_new=str.split(new_comment4,'¥')\n",
    "    #Step7:特征处理\n",
    "    test_tf_idf =tf_idf.transform(comment_new)\n",
    "    x=test_tf_idf\n",
    "    x=Standard_demo(x)\n",
    "    prediction=vote.predict(x)\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=test_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata1['prediction']=list(prediction)[0:-1]\n",
    "testdata1=testdata1[testdata1['prediction']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_demo():\n",
    "    comment=text_join(testdata1)\n",
    "    #Step1:删除数据中的@部分、&部分以及网址\n",
    "    new_comment=drop_words(comment)\n",
    "    #Step2:拼写检查更正\n",
    "    new_comment1=spellcheck(new_comment)\n",
    "    #Step3:缩略词还原\n",
    "    new_comment2=cont_reback(new_comment1)\n",
    "    #Step4:词形还原\n",
    "    new_comment3=wordlemmatize(new_comment2)\n",
    "    #Step5:转化为小写\n",
    "    new_comment3=new_comment3.lower()                                                                                       \n",
    "    #Step6:去除标点符号和数字\n",
    "    new_comment4=exclude_word(new_comment3)\n",
    "    comment_new=str.split(new_comment4,'¥')\n",
    "    #Step7:特征处理\n",
    "    test_tf_idf =tf_idf.transform(comment_new)\n",
    "    x=test_tf_idf\n",
    "    x=Standard_demo(x)\n",
    "    prediction=vote.predict(x)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=test_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata1['prediction']=list(prediction)[0:-1]\n",
    "testdata1=testdata1[testdata1['prediction']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata1=testdata1.iloc[:,[1,3]].set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata1.to_csv('testdata1_new_complain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
